{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvec = open(\"x_train_TITLES.p\",\"r\")\n",
    "data  = pkl.load(wordvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "y = open(\"y_train_TITLES.p\",\"r\")\n",
    "labels = pkl.load(y)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxIter = 1000\n",
    "batch_size = 200\n",
    "learningRate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mlp(input_data):\n",
    "    \n",
    "    w1 = tf.get_variable(\"w0\",[data.shape[1], 20], initializer=tf.random_normal_initializer())\n",
    "    b1 = tf.get_variable(\"b0\",[20], initializer=tf.constant_initializer(0.0))\n",
    "    w2 = tf.get_variable(\"w1\",[20, 15], initializer=tf.random_normal_initializer())\n",
    "    b2 = tf.get_variable(\"b1\",[15], initializer=tf.constant_initializer(0.0))\n",
    "    w3 = tf.get_variable(\"w2\",[15, 2], initializer=tf.random_normal_initializer())\n",
    "    b3 = tf.get_variable(\"b2\",[2], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    fc1 = tf.nn.relu(tf.add(tf.matmul(input_data,w1),b1))\n",
    "    fc2 = tf.nn.relu(tf.add(tf.matmul(fc1,w2),b2))\n",
    "    fc3 = tf.nn.relu(tf.add(tf.matmul(fc2,w3),b3))\n",
    "    \n",
    "    weights = [w1,b1,w2,b2,w3,b3]\n",
    "    \n",
    "    return fc3, weights\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, data.shape[1]])\n",
    "Y = tf.placeholder(\"float\", [None, 2])\n",
    "\n",
    "pred, weights = mlp(X)\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(pred, Y)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "new_labels = []\n",
    "for i in labels:\n",
    "    temp = np.ones(2)\n",
    "    temp[i] = 0\n",
    "    new_labels.append(temp)\n",
    "\n",
    "new_labels = np.array(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "right_pred = tf.equal(tf.argmax(Y,1),tf.argmax(pred,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(right_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.478260874748\n",
      "step 2, training accuracy 0.565217375755\n",
      "step 4, training accuracy 0.565217375755\n",
      "step 6, training accuracy 0.565217375755\n",
      "step 8, training accuracy 0.565217375755\n",
      "step 10, training accuracy 0.565217375755\n",
      "step 12, training accuracy 0.565217375755\n",
      "step 14, training accuracy 0.695652186871\n",
      "step 16, training accuracy 0.739130437374\n",
      "step 18, training accuracy 0.739130437374\n",
      "step 20, training accuracy 0.739130437374\n",
      "step 22, training accuracy 0.782608687878\n",
      "step 24, training accuracy 0.826086938381\n",
      "step 26, training accuracy 0.826086938381\n",
      "step 28, training accuracy 0.826086938381\n",
      "step 30, training accuracy 0.826086938381\n",
      "step 32, training accuracy 0.826086938381\n",
      "step 34, training accuracy 0.826086938381\n",
      "step 36, training accuracy 0.826086938381\n",
      "step 38, training accuracy 0.826086938381\n",
      "step 40, training accuracy 0.826086938381\n",
      "step 42, training accuracy 0.826086938381\n",
      "step 44, training accuracy 0.826086938381\n",
      "step 46, training accuracy 0.826086938381\n",
      "step 48, training accuracy 0.826086938381\n",
      "step 50, training accuracy 0.913043498993\n",
      "step 52, training accuracy 0.913043498993\n",
      "step 54, training accuracy 0.913043498993\n",
      "step 56, training accuracy 0.913043498993\n",
      "step 58, training accuracy 0.913043498993\n",
      "step 60, training accuracy 0.913043498993\n",
      "step 62, training accuracy 0.913043498993\n",
      "step 64, training accuracy 0.913043498993\n",
      "step 66, training accuracy 0.913043498993\n",
      "step 68, training accuracy 0.913043498993\n",
      "step 70, training accuracy 0.956521749496\n",
      "step 72, training accuracy 0.956521749496\n",
      "step 74, training accuracy 0.956521749496\n",
      "step 76, training accuracy 0.956521749496\n",
      "step 78, training accuracy 0.956521749496\n",
      "step 80, training accuracy 0.956521749496\n",
      "step 82, training accuracy 0.956521749496\n",
      "step 84, training accuracy 0.956521749496\n",
      "step 86, training accuracy 0.956521749496\n",
      "step 88, training accuracy 0.956521749496\n",
      "step 90, training accuracy 0.956521749496\n",
      "step 92, training accuracy 0.956521749496\n",
      "step 94, training accuracy 0.956521749496\n",
      "step 96, training accuracy 0.956521749496\n",
      "step 98, training accuracy 0.956521749496\n",
      "step 100, training accuracy 0.956521749496\n",
      "step 102, training accuracy 0.956521749496\n",
      "step 104, training accuracy 0.956521749496\n",
      "step 106, training accuracy 0.956521749496\n",
      "step 108, training accuracy 0.956521749496\n",
      "step 110, training accuracy 0.956521749496\n",
      "step 112, training accuracy 0.956521749496\n",
      "step 114, training accuracy 0.956521749496\n",
      "step 116, training accuracy 0.956521749496\n",
      "step 118, training accuracy 0.956521749496\n",
      "step 120, training accuracy 0.956521749496\n",
      "step 122, training accuracy 0.956521749496\n",
      "step 124, training accuracy 0.956521749496\n",
      "step 126, training accuracy 0.956521749496\n",
      "step 128, training accuracy 0.956521749496\n",
      "step 130, training accuracy 0.956521749496\n",
      "step 132, training accuracy 0.956521749496\n",
      "step 134, training accuracy 0.956521749496\n",
      "step 136, training accuracy 0.956521749496\n",
      "step 138, training accuracy 0.956521749496\n",
      "step 140, training accuracy 0.956521749496\n",
      "step 142, training accuracy 0.956521749496\n",
      "step 144, training accuracy 0.956521749496\n",
      "step 146, training accuracy 0.956521749496\n",
      "step 148, training accuracy 0.956521749496\n",
      "step 150, training accuracy 0.956521749496\n",
      "step 152, training accuracy 0.956521749496\n",
      "step 154, training accuracy 0.956521749496\n",
      "step 156, training accuracy 0.956521749496\n",
      "step 158, training accuracy 0.956521749496\n",
      "step 160, training accuracy 0.956521749496\n",
      "step 162, training accuracy 0.956521749496\n",
      "step 164, training accuracy 0.956521749496\n",
      "step 166, training accuracy 0.956521749496\n",
      "step 168, training accuracy 0.956521749496\n",
      "step 170, training accuracy 0.956521749496\n",
      "step 172, training accuracy 0.956521749496\n",
      "step 174, training accuracy 0.956521749496\n",
      "step 176, training accuracy 0.956521749496\n",
      "step 178, training accuracy 0.956521749496\n",
      "step 180, training accuracy 0.956521749496\n",
      "step 182, training accuracy 0.956521749496\n",
      "step 184, training accuracy 0.956521749496\n",
      "step 186, training accuracy 0.956521749496\n",
      "step 188, training accuracy 0.956521749496\n",
      "step 190, training accuracy 0.956521749496\n",
      "step 192, training accuracy 0.956521749496\n",
      "step 194, training accuracy 0.956521749496\n",
      "step 196, training accuracy 0.956521749496\n",
      "step 198, training accuracy 0.956521749496\n",
      "step 200, training accuracy 0.956521749496\n",
      "step 202, training accuracy 0.956521749496\n",
      "step 204, training accuracy 0.956521749496\n",
      "step 206, training accuracy 0.956521749496\n",
      "step 208, training accuracy 0.956521749496\n",
      "step 210, training accuracy 0.956521749496\n",
      "step 212, training accuracy 0.956521749496\n",
      "step 214, training accuracy 0.956521749496\n",
      "step 216, training accuracy 0.956521749496\n",
      "step 218, training accuracy 0.956521749496\n",
      "step 220, training accuracy 0.956521749496\n",
      "step 222, training accuracy 0.956521749496\n",
      "step 224, training accuracy 0.956521749496\n",
      "step 226, training accuracy 0.956521749496\n",
      "step 228, training accuracy 0.956521749496\n",
      "step 230, training accuracy 0.956521749496\n",
      "step 232, training accuracy 0.956521749496\n",
      "step 234, training accuracy 0.956521749496\n",
      "step 236, training accuracy 0.956521749496\n",
      "step 238, training accuracy 0.956521749496\n",
      "step 240, training accuracy 0.956521749496\n",
      "step 242, training accuracy 0.956521749496\n",
      "step 244, training accuracy 0.956521749496\n",
      "step 246, training accuracy 0.956521749496\n",
      "step 248, training accuracy 0.956521749496\n",
      "step 250, training accuracy 0.956521749496\n",
      "step 252, training accuracy 0.956521749496\n",
      "step 254, training accuracy 0.956521749496\n",
      "step 256, training accuracy 0.956521749496\n",
      "step 258, training accuracy 0.956521749496\n",
      "step 260, training accuracy 0.956521749496\n",
      "step 262, training accuracy 0.956521749496\n",
      "step 264, training accuracy 0.956521749496\n",
      "step 266, training accuracy 0.956521749496\n",
      "step 268, training accuracy 0.956521749496\n",
      "step 270, training accuracy 0.956521749496\n",
      "step 272, training accuracy 0.956521749496\n",
      "step 274, training accuracy 0.956521749496\n",
      "step 276, training accuracy 0.956521749496\n",
      "step 278, training accuracy 0.956521749496\n",
      "step 280, training accuracy 0.956521749496\n",
      "step 282, training accuracy 0.956521749496\n",
      "step 284, training accuracy 0.956521749496\n",
      "step 286, training accuracy 0.956521749496\n",
      "step 288, training accuracy 0.956521749496\n",
      "step 290, training accuracy 0.956521749496\n",
      "step 292, training accuracy 0.956521749496\n",
      "step 294, training accuracy 0.956521749496\n",
      "step 296, training accuracy 0.956521749496\n",
      "step 298, training accuracy 0.956521749496\n",
      "step 300, training accuracy 0.956521749496\n",
      "step 302, training accuracy 0.956521749496\n",
      "step 304, training accuracy 0.956521749496\n",
      "step 306, training accuracy 0.956521749496\n",
      "step 308, training accuracy 0.956521749496\n",
      "step 310, training accuracy 0.956521749496\n",
      "step 312, training accuracy 0.956521749496\n",
      "step 314, training accuracy 0.956521749496\n",
      "step 316, training accuracy 0.956521749496\n",
      "step 318, training accuracy 0.956521749496\n",
      "step 320, training accuracy 0.956521749496\n",
      "step 322, training accuracy 0.956521749496\n",
      "step 324, training accuracy 0.956521749496\n",
      "step 326, training accuracy 0.956521749496\n",
      "step 328, training accuracy 0.956521749496\n",
      "step 330, training accuracy 0.956521749496\n",
      "step 332, training accuracy 0.956521749496\n",
      "step 334, training accuracy 0.956521749496\n",
      "step 336, training accuracy 0.956521749496\n",
      "step 338, training accuracy 0.956521749496\n",
      "step 340, training accuracy 0.956521749496\n",
      "step 342, training accuracy 0.956521749496\n",
      "step 344, training accuracy 0.956521749496\n",
      "step 346, training accuracy 0.956521749496\n",
      "step 348, training accuracy 0.956521749496\n",
      "step 350, training accuracy 0.956521749496\n",
      "step 352, training accuracy 0.956521749496\n",
      "step 354, training accuracy 0.956521749496\n",
      "step 356, training accuracy 0.956521749496\n",
      "step 358, training accuracy 0.956521749496\n",
      "step 360, training accuracy 0.956521749496\n",
      "step 362, training accuracy 0.956521749496\n",
      "step 364, training accuracy 0.956521749496\n",
      "step 366, training accuracy 0.956521749496\n",
      "step 368, training accuracy 0.956521749496\n",
      "step 370, training accuracy 0.956521749496\n",
      "step 372, training accuracy 0.956521749496\n",
      "step 374, training accuracy 0.956521749496\n",
      "step 376, training accuracy 0.956521749496\n",
      "step 378, training accuracy 0.956521749496\n",
      "step 380, training accuracy 0.956521749496\n",
      "step 382, training accuracy 0.956521749496\n",
      "step 384, training accuracy 0.956521749496\n",
      "step 386, training accuracy 0.956521749496\n",
      "step 388, training accuracy 0.956521749496\n",
      "step 390, training accuracy 0.956521749496\n",
      "step 392, training accuracy 0.956521749496\n",
      "step 394, training accuracy 0.956521749496\n",
      "step 396, training accuracy 0.956521749496\n",
      "step 398, training accuracy 0.956521749496\n",
      "step 400, training accuracy 0.956521749496\n",
      "step 402, training accuracy 0.956521749496\n",
      "step 404, training accuracy 0.956521749496\n",
      "step 406, training accuracy 0.956521749496\n",
      "step 408, training accuracy 0.956521749496\n",
      "step 410, training accuracy 0.956521749496\n",
      "step 412, training accuracy 0.956521749496\n",
      "step 414, training accuracy 0.956521749496\n",
      "step 416, training accuracy 0.956521749496\n",
      "step 418, training accuracy 0.956521749496\n",
      "step 420, training accuracy 0.956521749496\n",
      "step 422, training accuracy 0.956521749496\n",
      "step 424, training accuracy 0.956521749496\n",
      "step 426, training accuracy 0.956521749496\n",
      "step 428, training accuracy 0.956521749496\n",
      "step 430, training accuracy 0.956521749496\n",
      "step 432, training accuracy 0.956521749496\n",
      "step 434, training accuracy 0.956521749496\n",
      "step 436, training accuracy 0.956521749496\n",
      "step 438, training accuracy 0.956521749496\n",
      "step 440, training accuracy 0.956521749496\n",
      "step 442, training accuracy 0.956521749496\n",
      "step 444, training accuracy 0.956521749496\n",
      "step 446, training accuracy 0.956521749496\n",
      "step 448, training accuracy 0.956521749496\n",
      "step 450, training accuracy 0.956521749496\n",
      "step 452, training accuracy 0.956521749496\n",
      "step 454, training accuracy 0.956521749496\n",
      "step 456, training accuracy 0.956521749496\n",
      "step 458, training accuracy 0.956521749496\n",
      "step 460, training accuracy 0.956521749496\n",
      "step 462, training accuracy 0.956521749496\n",
      "step 464, training accuracy 0.956521749496\n",
      "step 466, training accuracy 0.956521749496\n",
      "step 468, training accuracy 0.956521749496\n",
      "step 470, training accuracy 0.956521749496\n",
      "step 472, training accuracy 0.956521749496\n",
      "step 474, training accuracy 0.956521749496\n",
      "step 476, training accuracy 0.956521749496\n",
      "step 478, training accuracy 0.956521749496\n",
      "step 480, training accuracy 0.956521749496\n",
      "step 482, training accuracy 0.956521749496\n",
      "step 484, training accuracy 0.956521749496\n",
      "step 486, training accuracy 0.956521749496\n",
      "step 488, training accuracy 0.956521749496\n",
      "step 490, training accuracy 0.956521749496\n",
      "step 492, training accuracy 0.956521749496\n",
      "step 494, training accuracy 0.956521749496\n",
      "step 496, training accuracy 0.956521749496\n",
      "step 498, training accuracy 0.956521749496\n",
      "step 500, training accuracy 0.956521749496\n",
      "step 502, training accuracy 0.956521749496\n",
      "step 504, training accuracy 0.956521749496\n",
      "step 506, training accuracy 0.956521749496\n",
      "step 508, training accuracy 0.956521749496\n",
      "step 510, training accuracy 0.956521749496\n",
      "step 512, training accuracy 0.956521749496\n",
      "step 514, training accuracy 0.956521749496\n",
      "step 516, training accuracy 0.956521749496\n",
      "step 518, training accuracy 0.956521749496\n",
      "step 520, training accuracy 0.956521749496\n",
      "step 522, training accuracy 0.956521749496\n",
      "step 524, training accuracy 0.956521749496\n",
      "step 526, training accuracy 0.956521749496\n",
      "step 528, training accuracy 0.956521749496\n",
      "step 530, training accuracy 0.956521749496\n",
      "step 532, training accuracy 0.956521749496\n",
      "step 534, training accuracy 0.956521749496\n",
      "step 536, training accuracy 0.956521749496\n",
      "step 538, training accuracy 0.956521749496\n",
      "step 540, training accuracy 0.956521749496\n",
      "step 542, training accuracy 0.956521749496\n",
      "step 544, training accuracy 0.956521749496\n",
      "step 546, training accuracy 0.956521749496\n",
      "step 548, training accuracy 0.956521749496\n",
      "step 550, training accuracy 0.956521749496\n",
      "step 552, training accuracy 0.956521749496\n",
      "step 554, training accuracy 0.956521749496\n",
      "step 556, training accuracy 0.956521749496\n",
      "step 558, training accuracy 0.956521749496\n",
      "step 560, training accuracy 0.956521749496\n",
      "step 562, training accuracy 0.956521749496\n",
      "step 564, training accuracy 0.956521749496\n",
      "step 566, training accuracy 0.956521749496\n",
      "step 568, training accuracy 0.956521749496\n",
      "step 570, training accuracy 0.956521749496\n",
      "step 572, training accuracy 0.956521749496\n",
      "step 574, training accuracy 0.956521749496\n",
      "step 576, training accuracy 0.956521749496\n",
      "step 578, training accuracy 0.956521749496\n",
      "step 580, training accuracy 0.956521749496\n",
      "step 582, training accuracy 0.956521749496\n",
      "step 584, training accuracy 0.956521749496\n",
      "step 586, training accuracy 0.956521749496\n",
      "step 588, training accuracy 0.956521749496\n",
      "step 590, training accuracy 0.956521749496\n",
      "step 592, training accuracy 0.956521749496\n",
      "step 594, training accuracy 0.956521749496\n",
      "step 596, training accuracy 0.956521749496\n",
      "step 598, training accuracy 0.956521749496\n",
      "step 600, training accuracy 0.956521749496\n",
      "step 602, training accuracy 0.956521749496\n",
      "step 604, training accuracy 0.956521749496\n",
      "step 606, training accuracy 0.956521749496\n",
      "step 608, training accuracy 0.956521749496\n",
      "step 610, training accuracy 0.956521749496\n",
      "step 612, training accuracy 0.956521749496\n",
      "step 614, training accuracy 0.956521749496\n",
      "step 616, training accuracy 0.956521749496\n",
      "step 618, training accuracy 0.956521749496\n",
      "step 620, training accuracy 0.956521749496\n",
      "step 622, training accuracy 0.956521749496\n",
      "step 624, training accuracy 0.956521749496\n",
      "step 626, training accuracy 0.956521749496\n",
      "step 628, training accuracy 0.956521749496\n",
      "step 630, training accuracy 0.956521749496\n",
      "step 632, training accuracy 0.956521749496\n",
      "step 634, training accuracy 0.956521749496\n",
      "step 636, training accuracy 0.956521749496\n",
      "step 638, training accuracy 0.956521749496\n",
      "step 640, training accuracy 0.956521749496\n",
      "step 642, training accuracy 0.956521749496\n",
      "step 644, training accuracy 0.956521749496\n",
      "step 646, training accuracy 0.956521749496\n",
      "step 648, training accuracy 0.956521749496\n",
      "step 650, training accuracy 0.956521749496\n",
      "step 652, training accuracy 0.956521749496\n",
      "step 654, training accuracy 0.956521749496\n",
      "step 656, training accuracy 0.956521749496\n",
      "step 658, training accuracy 0.956521749496\n",
      "step 660, training accuracy 0.956521749496\n",
      "step 662, training accuracy 0.956521749496\n",
      "step 664, training accuracy 0.956521749496\n",
      "step 666, training accuracy 0.956521749496\n",
      "step 668, training accuracy 0.956521749496\n",
      "step 670, training accuracy 0.956521749496\n",
      "step 672, training accuracy 0.956521749496\n",
      "step 674, training accuracy 0.956521749496\n",
      "step 676, training accuracy 0.956521749496\n",
      "step 678, training accuracy 0.956521749496\n",
      "step 680, training accuracy 0.956521749496\n",
      "step 682, training accuracy 0.956521749496\n",
      "step 684, training accuracy 0.956521749496\n",
      "step 686, training accuracy 0.956521749496\n",
      "step 688, training accuracy 0.956521749496\n",
      "step 690, training accuracy 0.956521749496\n",
      "step 692, training accuracy 0.956521749496\n",
      "step 694, training accuracy 0.956521749496\n",
      "step 696, training accuracy 0.956521749496\n",
      "step 698, training accuracy 0.956521749496\n",
      "step 700, training accuracy 0.956521749496\n",
      "step 702, training accuracy 0.956521749496\n",
      "step 704, training accuracy 0.956521749496\n",
      "step 706, training accuracy 0.956521749496\n",
      "step 708, training accuracy 0.956521749496\n",
      "step 710, training accuracy 0.956521749496\n",
      "step 712, training accuracy 0.956521749496\n",
      "step 714, training accuracy 0.956521749496\n",
      "step 716, training accuracy 0.956521749496\n",
      "step 718, training accuracy 0.956521749496\n",
      "step 720, training accuracy 0.956521749496\n",
      "step 722, training accuracy 0.956521749496\n",
      "step 724, training accuracy 0.956521749496\n",
      "step 726, training accuracy 0.956521749496\n",
      "step 728, training accuracy 0.956521749496\n",
      "step 730, training accuracy 0.956521749496\n",
      "step 732, training accuracy 0.956521749496\n",
      "step 734, training accuracy 0.956521749496\n",
      "step 736, training accuracy 0.956521749496\n",
      "step 738, training accuracy 0.956521749496\n",
      "step 740, training accuracy 0.956521749496\n",
      "step 742, training accuracy 0.956521749496\n",
      "step 744, training accuracy 0.956521749496\n",
      "step 746, training accuracy 0.956521749496\n",
      "step 748, training accuracy 0.956521749496\n",
      "step 750, training accuracy 0.956521749496\n",
      "step 752, training accuracy 0.956521749496\n",
      "step 754, training accuracy 0.956521749496\n",
      "step 756, training accuracy 0.956521749496\n",
      "step 758, training accuracy 0.956521749496\n",
      "step 760, training accuracy 0.956521749496\n",
      "step 762, training accuracy 0.956521749496\n",
      "step 764, training accuracy 0.956521749496\n",
      "step 766, training accuracy 0.956521749496\n",
      "step 768, training accuracy 0.956521749496\n",
      "step 770, training accuracy 0.956521749496\n",
      "step 772, training accuracy 0.956521749496\n",
      "step 774, training accuracy 0.956521749496\n",
      "step 776, training accuracy 0.956521749496\n",
      "step 778, training accuracy 0.956521749496\n",
      "step 780, training accuracy 0.956521749496\n",
      "step 782, training accuracy 0.956521749496\n",
      "step 784, training accuracy 0.956521749496\n",
      "step 786, training accuracy 0.956521749496\n",
      "step 788, training accuracy 0.956521749496\n",
      "step 790, training accuracy 0.956521749496\n",
      "step 792, training accuracy 0.956521749496\n",
      "step 794, training accuracy 0.956521749496\n",
      "step 796, training accuracy 0.956521749496\n",
      "step 798, training accuracy 0.956521749496\n",
      "step 800, training accuracy 0.956521749496\n",
      "step 802, training accuracy 0.956521749496\n",
      "step 804, training accuracy 0.956521749496\n",
      "step 806, training accuracy 0.956521749496\n",
      "step 808, training accuracy 0.956521749496\n",
      "step 810, training accuracy 0.956521749496\n",
      "step 812, training accuracy 0.956521749496\n",
      "step 814, training accuracy 0.956521749496\n",
      "step 816, training accuracy 0.956521749496\n",
      "step 818, training accuracy 0.956521749496\n",
      "step 820, training accuracy 0.956521749496\n",
      "step 822, training accuracy 0.956521749496\n",
      "step 824, training accuracy 0.956521749496\n",
      "step 826, training accuracy 0.956521749496\n",
      "step 828, training accuracy 0.956521749496\n",
      "step 830, training accuracy 0.956521749496\n",
      "step 832, training accuracy 0.956521749496\n",
      "step 834, training accuracy 0.956521749496\n",
      "step 836, training accuracy 0.956521749496\n",
      "step 838, training accuracy 0.956521749496\n",
      "step 840, training accuracy 0.956521749496\n",
      "step 842, training accuracy 0.956521749496\n",
      "step 844, training accuracy 0.956521749496\n",
      "step 846, training accuracy 0.956521749496\n",
      "step 848, training accuracy 0.956521749496\n",
      "step 850, training accuracy 0.956521749496\n",
      "step 852, training accuracy 0.956521749496\n",
      "step 854, training accuracy 0.956521749496\n",
      "step 856, training accuracy 0.956521749496\n",
      "step 858, training accuracy 0.956521749496\n",
      "step 860, training accuracy 0.956521749496\n",
      "step 862, training accuracy 0.956521749496\n",
      "step 864, training accuracy 0.956521749496\n",
      "step 866, training accuracy 0.956521749496\n",
      "step 868, training accuracy 0.956521749496\n",
      "step 870, training accuracy 0.956521749496\n",
      "step 872, training accuracy 0.956521749496\n",
      "step 874, training accuracy 0.956521749496\n",
      "step 876, training accuracy 0.956521749496\n",
      "step 878, training accuracy 0.956521749496\n",
      "step 880, training accuracy 0.913043498993\n",
      "step 882, training accuracy 0.956521749496\n",
      "step 884, training accuracy 0.956521749496\n",
      "step 886, training accuracy 0.956521749496\n",
      "step 888, training accuracy 0.956521749496\n",
      "step 890, training accuracy 0.956521749496\n",
      "step 892, training accuracy 0.956521749496\n",
      "step 894, training accuracy 0.956521749496\n",
      "step 896, training accuracy 0.956521749496\n",
      "step 898, training accuracy 0.956521749496\n",
      "step 900, training accuracy 0.956521749496\n",
      "step 902, training accuracy 0.956521749496\n",
      "step 904, training accuracy 0.956521749496\n",
      "step 906, training accuracy 0.956521749496\n",
      "step 908, training accuracy 0.956521749496\n",
      "step 910, training accuracy 0.956521749496\n",
      "step 912, training accuracy 0.956521749496\n",
      "step 914, training accuracy 0.956521749496\n",
      "step 916, training accuracy 0.956521749496\n",
      "step 918, training accuracy 0.956521749496\n",
      "step 920, training accuracy 0.956521749496\n",
      "step 922, training accuracy 0.956521749496\n",
      "step 924, training accuracy 0.956521749496\n",
      "step 926, training accuracy 0.956521749496\n",
      "step 928, training accuracy 0.956521749496\n",
      "step 930, training accuracy 0.956521749496\n",
      "step 932, training accuracy 0.956521749496\n",
      "step 934, training accuracy 0.956521749496\n",
      "step 936, training accuracy 0.956521749496\n",
      "step 938, training accuracy 0.956521749496\n",
      "step 940, training accuracy 0.956521749496\n",
      "step 942, training accuracy 0.956521749496\n",
      "step 944, training accuracy 0.956521749496\n",
      "step 946, training accuracy 0.956521749496\n",
      "step 948, training accuracy 0.956521749496\n",
      "step 950, training accuracy 0.956521749496\n",
      "step 952, training accuracy 0.956521749496\n",
      "step 954, training accuracy 0.956521749496\n",
      "step 956, training accuracy 0.956521749496\n",
      "step 958, training accuracy 0.956521749496\n",
      "step 960, training accuracy 0.956521749496\n",
      "step 962, training accuracy 0.956521749496\n",
      "step 964, training accuracy 0.956521749496\n",
      "step 966, training accuracy 0.956521749496\n",
      "step 968, training accuracy 0.956521749496\n",
      "step 970, training accuracy 0.956521749496\n",
      "step 972, training accuracy 0.956521749496\n",
      "step 974, training accuracy 0.956521749496\n",
      "step 976, training accuracy 0.956521749496\n",
      "step 978, training accuracy 0.956521749496\n",
      "step 980, training accuracy 0.956521749496\n",
      "step 982, training accuracy 0.956521749496\n",
      "step 984, training accuracy 0.956521749496\n",
      "step 986, training accuracy 0.956521749496\n",
      "step 988, training accuracy 0.956521749496\n",
      "step 990, training accuracy 0.956521749496\n",
      "step 992, training accuracy 0.956521749496\n",
      "step 994, training accuracy 0.956521749496\n",
      "step 996, training accuracy 0.956521749496\n",
      "step 998, training accuracy 0.956521749496\n"
     ]
    }
   ],
   "source": [
    "num_batch = data.shape[0]/batch_size + 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    for it in range(maxIter):\n",
    "        for i in range(num_batch):            \n",
    "            \n",
    "            start = i*batch_size\n",
    "            if (i == num_batch-1):\n",
    "                end = data.shape[0]\n",
    "            else:\n",
    "                end = (i+1)*batch_size\n",
    "            \n",
    "            input_data = data[start:end]\n",
    "            \n",
    "            label = new_labels[start:end]\n",
    "            \n",
    "            sess.run(optimizer, feed_dict={X: input_data, Y: label})\n",
    "            \n",
    "        if it % 2 == 0:\n",
    "            train_Accuracy = accuracy.eval(feed_dict={X: input_data, Y: label})\n",
    "\n",
    "            print('step {0}, training accuracy {1}'.format(it, train_Accuracy))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
